# pretraining-llm
based on transformer from 0 to 1 pretraining llm
